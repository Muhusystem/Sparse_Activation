# Achieving Sparse Activation in Small Language Models

## Introduction
This is the official code repository for the paper ["Achieving Sparse Activation in Small Language Models"](). Sparse activation selectively activates only an input-dependent set of neurons in inference, is a useful technique to reduce the computing cost of Large Language Models (LLMs) without retraining or adaptation efforts. We proposed a new attribution metric for Small Language Models (SLMs) that can achieve 80% sparsification ratio with $<$5% model accuracy loss.

## Requirement
Install all the required packages.
```
pip install -r requirements.txt
```
